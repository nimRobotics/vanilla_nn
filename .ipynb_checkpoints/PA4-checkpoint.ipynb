{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA_4: Feedforward Neural Network\n",
    "\n",
    "## Aim\n",
    "Train and test a Feedforward Neural Network for MNIST digit classification.\n",
    "\n",
    "## Procedure\n",
    "* Download `mnist_file.rar` which contains mnist data as a *pickle* file and read `mnist.py` for loading partial mnist data.\n",
    "* Run read `mnist.py` file which will give 1000 train and 500 test images per each class.\n",
    "* x train,y train gives the image $784\\times1$ and corresponding label for training data. Similarly, for test data.\n",
    "* Write\n",
    "1. Neural network model using library functions.\n",
    "2. Your own neural network model and train with Back propagation\n",
    "    1. On the training data and report accuracy.\n",
    "    2. Train with Five fold cross validation (4 fold training and 1 fold testing. Repeating this for 5 times changing the test fold each time) and report the average accuracy as train accuracy.\n",
    "* Test both models with the test data.\n",
    "* Find the confusion matrix and report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data label dim: (10000,)\n",
      "Train data features dim: (10000, 784)\n",
      "Test data label dim: (5000,)\n",
      "Test data features dim:(5000, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from utils import visualise\n",
    "from read_mnist import load_data\n",
    "import random\n",
    "\n",
    "y_train,x_train,y_test,x_test=load_data()\n",
    "\n",
    "print(\"Train data label dim: {}\".format(y_train.shape))\n",
    "print(\"Train data features dim: {}\".format(x_train.shape))\n",
    "print(\"Test data label dim: {}\".format(y_test.shape))\n",
    "print(\"Test data features dim:{}\".format(x_test.shape))\n",
    "\n",
    "# visualise(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 784) (8000,)\n",
      "Training started (validation)!\n",
      "Loss: 2.2975906073733006 Training progress: 0/200\n",
      "Loss: 1.8984211244502607 Training progress: 100/200\n",
      "(8000, 784) (8000,)\n",
      "Training started (validation)!\n",
      "Loss: 2.2963204861817776 Training progress: 0/200\n",
      "Loss: 1.9037190510614275 Training progress: 100/200\n",
      "(8000, 784) (8000,)\n",
      "Training started (validation)!\n",
      "Loss: 2.2974862330560573 Training progress: 0/200\n",
      "Loss: 1.8906052405815312 Training progress: 100/200\n",
      "(8000, 784) (8000,)\n",
      "Training started (validation)!\n",
      "Loss: 2.302525404337499 Training progress: 0/200\n",
      "Loss: 1.866812125912864 Training progress: 100/200\n",
      "[0.698875, 0.69425, 0.687125, 0.667375]\n",
      "Train Results-\n",
      " Accuracy: 0.68690625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from read_mnist import load_data\n",
    "import pickle\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "    return 1-np.power(x,2)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x /np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return (x>0)*1\n",
    "\n",
    "def cross_entropy(y_,y):\n",
    "    n = y.shape[0]\n",
    "    nll = -np.log(y_[range(n),y])\n",
    "    return np.mean(nll)\n",
    "\n",
    "def delta_cross_entropy(y_,y):\n",
    "    n = y.shape[0]\n",
    "    y_[range(n),y] -= 1\n",
    "    return y_/n\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, hidden_layers, hidden_neurons, hidden_activation, lr=0.01):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.lr=lr\n",
    "        np.random.seed(786)\n",
    "        self.W1 = 0.1* np.random.randn(x_train.shape[1],self.hidden_neurons)\n",
    "        self.b1 = np.zeros((1,self.hidden_neurons))\n",
    "        self.W2 = 0.1* np.random.randn(self.hidden_neurons,10)\n",
    "        self.b2 = np.zeros((1,10))\n",
    "\n",
    "    def forward(self,x_train):\n",
    "        s1=np.dot(x_train, self.W1) + self.b1\n",
    "        if self.hidden_activation == 'sigmoid':\n",
    "            a1 = sigmoid(s1)\n",
    "        elif self.hidden_activation=='tanh':\n",
    "            a1 = np.tanh(s1)\n",
    "        elif self.hidden_activation=='relu':\n",
    "            a1 = relu(s1)\n",
    "        else:\n",
    "            raise Exception('Error: Activation not implemented')\n",
    "        s2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = softmax(s2)\n",
    "        loss=cross_entropy(a2,y_train)\n",
    "        return(loss,s1,a1,s2,a2)\n",
    "\n",
    "\n",
    "    def backward(self, s1, a1, s2, a2):\n",
    "        delta3=delta_cross_entropy(a2,y_train)\n",
    "        dW2 = np.dot(a1.T, delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        if self.hidden_activation=='sigmoid':\n",
    "            delta2 = delta3.dot(self.W2.T) * sigmoid_grad(a1)\n",
    "        elif self.hidden_activation == 'tanh':\n",
    "            delta2 = delta3.dot(self.W2.T) * tanh_grad(a1)\n",
    "        elif self.hidden_activation == 'relu':\n",
    "            delta2 = delta3.dot(self.W2.T) * relu_grad(a1)\n",
    "        else:\n",
    "            raise Exception('Error: Activation not implemented')\n",
    "            \n",
    "        dW1 = np.dot(x_train.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        self.W1 += -self.lr * dW1\n",
    "        self.b1 += -self.lr * db1\n",
    "        self.W2 += -self.lr * dW2\n",
    "        self.b2 += -self.lr * db2\n",
    "        \n",
    "    def predict(self, x):\n",
    "        s1=np.dot(x, self.W1)\n",
    "        a1 = (sigmoid(s1))\n",
    "        s2 = np.dot(a1, self.W2)\n",
    "        a2 = softmax(s2)\n",
    "        return np.argmax(a2, axis=1)\n",
    "    \n",
    "    def save_model(self, name):\n",
    "        params = { 'W1': self.W1, 'b1': self.b1, 'W2': self.W2, 'b2': self.b2}\n",
    "        with open(name, 'wb') as handle:\n",
    "            pickle.dump(params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "epochs=200\n",
    "# hyperparameter variation\n",
    "lr=0.1\n",
    "neurons = [32]\n",
    "# activations = ['sigmoid', 'relu', 'tanh']\n",
    "activations = ['sigmoid']\n",
    "\n",
    "experiments = list(itertools.product(neurons, activations))\n",
    "\n",
    "# for (hidden_neurons,hidden_activation) in experiments:\n",
    "#     print('############ Activation function: {} No. of neurons: {} ############'.format(hidden_activation, hidden_neurons))\n",
    "#     model=NN(hidden_layers=5,hidden_neurons=hidden_neurons,hidden_activation=hidden_activation, lr=lr)\n",
    "#     print('Training started!')\n",
    "#     start = time.time()\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         loss,s1,a1,s2,a2 = model.forward(x_train)\n",
    "#         if epoch%100==0:\n",
    "#             print(\"Loss: {} Training progress: {}/{}\".format(loss,epoch,epochs))\n",
    "#         model.backward(s1, a1, s2, a2)\n",
    "        \n",
    "#     name = 'model_'+str(hidden_activation)+'_'+str(hidden_neurons)+'.pickle'\n",
    "#     model.save_model(name=name)\n",
    "#     stop = time.time()\n",
    "    \n",
    "#     print('Training finished in {} s'.format(stop - start))\n",
    "#     test_preds = model.predict(x_test)\n",
    "#     print('Test Results-\\n Accuracy: {} F1-Score: {}, Precision: {} Recall: {}'.format( np.mean(test_preds == y_test), f1_score(y_test, test_preds, average='micro'), precision_score(y_test, test_preds, average='micro'), recall_score(y_test, test_preds, average='micro') ))\n",
    "\n",
    "\n",
    "    \n",
    "y_train_or = np.copy(y_train)\n",
    "x_train_or = np.copy(x_train)\n",
    "y_test_or = np.copy(y_test)\n",
    "x_test_or = np.copy(x_test)\n",
    "\n",
    "# five fold cross validation\n",
    "folds=5\n",
    "val_acc=[]\n",
    "for (hidden_neurons,hidden_activation) in experiments:\n",
    "    for fold in range(1,folds):\n",
    "        # x_train.shape[0]=10000\n",
    "        start=int(fold*(x_train_or.shape[0]/folds))\n",
    "        stop=int((fold+1)*(x_train_or.shape[0]/folds))\n",
    "        \n",
    "        del x_train\n",
    "        del y_train\n",
    "        del x_test\n",
    "        del y_test\n",
    "\n",
    "        x_test=x_train_or[start:stop]\n",
    "        y_test=y_train_or[start:stop]\n",
    "        \n",
    "\n",
    "        x_train=np.vstack((x_train_or[:start],  x_train_or[stop:]))\n",
    "        y_train=np.append(y_train_or[:start],y_train_or[stop:])\n",
    "        print(x_train.shape, y_train.shape)\n",
    "\n",
    "        model=NN(hidden_layers=5,hidden_neurons=hidden_neurons,hidden_activation=hidden_activation, lr=lr)\n",
    "        print('Training started (validation)!')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss,s1,a1,s2,a2 = model.forward(x_train)\n",
    "            if epoch%100==0:\n",
    "                print(\"Loss: {} Training progress: {}/{}\".format(loss,epoch,epochs))\n",
    "            model.backward(s1, a1, s2, a2)\n",
    "\n",
    "        train_preds= model.predict(x_train)\n",
    "        val_acc.append(np.mean(train_preds == y_train))\n",
    "    print(val_acc)\n",
    "    print('Train Results-\\n Accuracy: {}'.format(np.mean(np.array(val_acc)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_sigmoid_32.pickle\n",
      "model_relu_32.pickle\n",
      "model_tanh_32.pickle\n",
      "model_sigmoid_64.pickle\n",
      "model_relu_64.pickle\n",
      "model_tanh_64.pickle\n",
      "model_sigmoid_128.pickle\n",
      "model_relu_128.pickle\n",
      "model_tanh_128.pickle\n",
      "model_sigmoid_256.pickle\n",
      "model_relu_256.pickle\n",
      "model_tanh_256.pickle\n",
      "model_sigmoid_512.pickle\n",
      "model_relu_512.pickle\n",
      "model_tanh_512.pickle\n"
     ]
    }
   ],
   "source": [
    "# Load model parameters\n",
    "for (hidden_neurons,hidden_activation) in experiments:\n",
    "    name = 'model_'+str(hidden_activation)+'_'+str(hidden_neurons)+'.pickle'\n",
    "    print(name)\n",
    "    with open(name, 'rb') as handle:\n",
    "        b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
